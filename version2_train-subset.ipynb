{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as k\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "home = '/home/nandhini/aozel/periodicBox'\n",
    "phi = 0.1\n",
    "data_signature1 = 'fsize' + str(9)+'phi'+str(phi)+'dp'+str(75)+'timestep'+str(0.6)\n",
    "data_signature2 = 'fsize' + str(9)+'phi'+str(phi)+'dp'+str(75)+'timestep'+str(0.7)\n",
    "data_signature3 = 'fsize' + str(5)+'phi'+str(phi)+'dp'+str(75)+'timestep'+str(0.525)\n",
    "data_signature4 = 'fsize' + str(11)+'phi'+str(phi)+'dp'+str(75)+'timestep'+str(0.55)\n",
    "data_signature5 = 'fsize' + str(9)+'phi'+str(phi)+'dp'+str(150)+'timestep'+str(0.6)\n",
    "data_signature6 = 'fsize' + str(9)+'phi'+str(phi)+'dp'+str(300)+'timestep'+str(0.65) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_myData1 = \"corrected_myDataFSUslip\"+data_signature1+\".pickle\"\n",
    "file_myData2 = \"corrected_myDataFSUslip\"+data_signature2+\".pickle\"\n",
    "file_myData3 = \"corrected_myDataFSUslip\"+data_signature3+\".pickle\"\n",
    "file_myData4 = \"corrected_myDataFSUslip\"+data_signature4+\".pickle\"\n",
    "file_myData5 = \"corrected_myDataFSUslip\"+data_signature5+\".pickle\"\n",
    "file_myData6 = \"corrected_myDataFSUslip\"+data_signature6+\".pickle\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_target1 = \"corrected_targetFSUslip\"+data_signature1+\".pickle\"\n",
    "file_target2 = \"corrected_targetFSUslip\"+data_signature2+\".pickle\"\n",
    "file_target3 = \"corrected_targetFSUslip\"+data_signature3+\".pickle\"\n",
    "file_target4 = \"corrected_targetFSUslip\"+data_signature4+\".pickle\"\n",
    "file_target5 = \"corrected_targetFSUslip\"+data_signature5+\".pickle\"\n",
    "file_target6 = \"corrected_targetFSUslip\"+data_signature6+\".pickle\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "myData1 = pickle.load( open( file_myData1, \"rb\" ) )\n",
    "myData2 = pickle.load( open( file_myData2, \"rb\" ) )\n",
    "myData3 = pickle.load( open( file_myData3, \"rb\" ) )\n",
    "myData4 = pickle.load( open( file_myData4, \"rb\" ) )\n",
    "myData5 = pickle.load( open( file_myData5, \"rb\" ) )\n",
    "myData6 = pickle.load( open( file_myData6, \"rb\" ) )\n",
    "\n",
    "\n",
    "myData = np.vstack((myData1, myData2, myData3, myData4, myData5, myData6))\n",
    "                \n",
    "\n",
    "target1 = pickle.load( open( file_target1, \"rb\" ) )\n",
    "target2 = pickle.load( open( file_target2, \"rb\" ) )\n",
    "target3 = pickle.load( open( file_target3, \"rb\" ) )\n",
    "target4 = pickle.load( open( file_target4, \"rb\" ) )\n",
    "target5 = pickle.load( open( file_target5, \"rb\" ) )\n",
    "target6 = pickle.load( open( file_target6, \"rb\" ) )\n",
    "\n",
    "\n",
    "target = np.vstack((target1.reshape(-1,1),target2.reshape(-1,1),\n",
    "                    target3.reshape(-1,1),target4.reshape(-1,1),target5.reshape(-1,1),\n",
    "                    target6.reshape(-1,1)\n",
    "                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11608272, 109), (11608272, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_myData = \"corrected_myDataFSUslipdevset.pickle\"\n",
    "file_target = \"corrected_targetFSUslipdevset.pickle\"\n",
    "X_dev = pickle.load( open( file_myData, \"rb\" ) )\n",
    "y_dev = pickle.load( open( file_target, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = pickle.load( open( 'corrected_FSscale_version2', \"rb\" ) )\n",
    "X_train_norm = scaler.transform(myData)\n",
    "X_dev_norm = scaler.transform(X_dev)\n",
    "y_train = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.savetxt(\"drag_std.csv\", scaler.std_, delimiter=\",\")\n",
    "#np.savetxt(\"drag_mean.csv\", scaler.mean_, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11608272, 109), (77389, 109), (11608272, 1), (77389, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm.shape, X_dev_norm.shape, y_train.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train_norm, y_train = shuffle(X_train_norm, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_lists = [[4*x+1,4*x+4] for x in range(27)]\n",
    "flattened = [0] + [val for sublist in list_of_lists for val in sublist] \n",
    "len(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='normal',input_dim=len(flattened)))\n",
    "model.add(Dense(1, activation='linear', kernel_initializer='normal'))\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "save_file = 'corrected_version2_small_subset_phi_vz.hdf5'\n",
    "#model.load_weights(save_file)\n",
    "checkpointer = ModelCheckpoint(filepath=save_file, verbose=1, save_best_only=True)\n",
    "model.fit(X_train_norm[:,np.array(flattened)], y_train, batch_size=512, epochs=15, verbose=1, validation_data=(X_dev_norm[:,np.array(flattened)], y_dev), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_lists = [4*x+4 for x in range(27)]\n",
    "flattened = [0] + [53] + list_of_lists\n",
    "len(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='normal',input_dim=len(flattened)))\n",
    "model.add(Dense(1, activation='linear', kernel_initializer='normal'))\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "save_file = 'corrected_version2_small_subset_vz.hdf5'\n",
    "#model.load_weights(save_file)\n",
    "checkpointer = ModelCheckpoint(filepath=save_file, verbose=1, save_best_only=True)\n",
    "model.fit(X_train_norm[:,np.array(flattened)], y_train, batch_size=512, epochs=15, verbose=1, validation_data=(X_dev_norm[:,np.array(flattened)], y_dev), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_lists = [4*x+1 for x in range(27)]\n",
    "flattened = [0] + [56] + list_of_lists\n",
    "len(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='normal',input_dim=len(flattened)))\n",
    "model.add(Dense(1, activation='linear', kernel_initializer='normal'))\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "save_file = 'corrected_version2_small_subset_phi.hdf5'\n",
    "#model.load_weights(save_file)\n",
    "checkpointer = ModelCheckpoint(filepath=save_file, verbose=1, save_best_only=True)\n",
    "model.fit(X_train_norm[:,np.array(flattened)], y_train, batch_size=512, epochs=15, verbose=1, validation_data=(X_dev_norm[:,np.array(flattened)], y_dev), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alist = []\n",
    "index = 0\n",
    "for i in range(-1,2):\n",
    "    for j in range(-1,2):\n",
    "        for k in range(-1,2):\n",
    "            if (i == 0 and j == 0) or (i ==0 and k == 0) or (j == 0 and k == 0): \n",
    "                alist.append(index)\n",
    "            index +=1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_lists = [[4*x+1,4*x+4] for x in alist]\n",
    "flattened = [0] + [val for sublist in list_of_lists for val in sublist] \n",
    "#flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='normal',input_dim=len(flattened)))\n",
    "model.add(Dense(1, activation='linear', kernel_initializer='normal'))\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "save_file = 'corrected_version2_small_subset_phi_vz_seven_cells.hdf5'\n",
    "#model.load_weights(save_file)\n",
    "checkpointer = ModelCheckpoint(filepath=save_file, verbose=1, save_best_only=True)\n",
    "model.fit(X_train_norm[:,np.array(flattened)], y_train, batch_size=512, epochs=15, verbose=1, validation_data=(X_dev_norm[:,np.array(flattened)], y_dev), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_lists = [4*x+4 for x in alist]\n",
    "flattened = [0] + [53] + list_of_lists\n",
    "len(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='normal',input_dim=len(flattened)))\n",
    "model.add(Dense(1, activation='linear', kernel_initializer='normal'))\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "save_file = 'corrected_version2_small_subset_vz_seven_cells.hdf5'\n",
    "#model.load_weights(save_file)\n",
    "checkpointer = ModelCheckpoint(filepath=save_file, verbose=1, save_best_only=True)\n",
    "model.fit(X_train_norm[:,np.array(flattened)], y_train, batch_size=512, epochs=15, verbose=1, validation_data=(X_dev_norm[:,np.array(flattened)], y_dev), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_lists = [4*x+1 for x in alist]\n",
    "flattened = [0] + [56] + list_of_lists\n",
    "len(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='normal',input_dim=len(flattened)))\n",
    "model.add(Dense(1, activation='linear', kernel_initializer='normal'))\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "save_file = 'corrected_version2_small_subset_phi_seven_cells.hdf5'\n",
    "#model.load_weights(save_file)\n",
    "checkpointer = ModelCheckpoint(filepath=save_file, verbose=1, save_best_only=True)\n",
    "model.fit(X_train_norm[:,np.array(flattened)], y_train, batch_size=512, epochs=15, verbose=1, validation_data=(X_dev_norm[:,np.array(flattened)], y_dev), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gradients(a):\n",
    "    \"\"\"Average first and last element of a 1-D array\"\"\"\n",
    "    x_left   = np.hstack([a[0], a[3], a[6]])\n",
    "    x_middle = np.hstack([a[1], a[4], a[7]])\n",
    "    x_right  = np.hstack([a[2], a[5], a[8]])\n",
    "    y_0 = a[1]\n",
    "    y_first  = (x_right-x_left)\n",
    "    y_second = (x_right-2*x_middle + x_left)\n",
    "    return np.hstack([y_0, y_first, y_second])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newlist1 = []\n",
    "newlist2 = []\n",
    "newlist3 = []\n",
    "\n",
    "index = 0\n",
    "for i in range(-1,2):\n",
    "    for j in range(-1,2):\n",
    "        for k in range(-1,2):\n",
    "            if (i == 0 and j == 0) or (i ==0 and k == 0) or (j == 0 and k == 0): \n",
    "                if i == 0 and j == 0:\n",
    "                    newlist1.append(index)\n",
    "                if i == 0 and k == 0:\n",
    "                    newlist2.append(index)\n",
    "                if j == 0 and k == 0:\n",
    "                    newlist3.append(index)\n",
    "            index +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newlist = newlist1 + newlist2 + newlist3\n",
    "phis_index =  [4*x+1 for x in newlist]\n",
    "vzs_index =   [4*x+4 for x in newlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([49, 53, 57, 41, 53, 65, 17, 53, 89], [52, 56, 60, 44, 56, 68, 20, 56, 92])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phis_index, vzs_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phis_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_gradients(X):\n",
    "    return np.hstack([X[:,:1], np.apply_along_axis(get_gradients, 1, X[:,phis_index]), \n",
    "           np.apply_along_axis(get_gradients, 1, X[:,vzs_index])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_norm_gradients = get_X_gradients(X_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 15)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm_gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_norm[:,np.array(flattened_part)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_func(a):\n",
    "    \"\"\"Average first and last element of a 1-D array\"\"\"\n",
    "    return np.array([a[0], a[-1]])\n",
    "b = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "np.apply_along_axis(my_func, 1, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
